"""
LangGraph ê¸°ë°˜ Talent Situational ì‹¬í™” ì§ˆë¬¸ ìƒì„±
ê¸°ì¡´ generate_deep_dive_question ë¡œì§ì„ LangGraphë¡œ ì „í™˜
"""

from typing import List, TypedDict, Literal
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field

from config.settings import get_settings


# ==================== State ì •ì˜ ====================

class TalentSituationalQuestionState(TypedDict):
    """Talent Situational ì‹¬í™” ì§ˆë¬¸ ìƒì„± State"""
    # Input
    dominant_trait: str  # ê°€ì¥ ê°•í•œ ì„±í–¥
    dimension: str  # í•´ë‹¹ ì°¨ì›
    qa_history: List[dict]  # ì´ì „ ì§ˆë¬¸-ë‹µë³€ ì´ë ¥

    # Process
    generated_question: str
    validation_errors: List[str]
    attempts: int
    llm_feedback: str

    # Output
    final_question: str
    is_valid: bool


# ==================== Generator Node ====================

def generate_situational_deep_dive_question_node(state: TalentSituationalQuestionState) -> TalentSituationalQuestionState:
    """
    Situational ì‹¬í™” ì§ˆë¬¸ ìƒì„± ë…¸ë“œ

    ê¸°ì¡´ talent/situational.pyì˜ generate_deep_dive_question ë¡œì§ ì‚¬ìš©
    """
    print(f"[Generator] Generating Situational deep-dive question (attempt {state['attempts'] + 1})")

    dominant_trait = state["dominant_trait"]
    dimension = state["dimension"]
    qa_history = state["qa_history"]

    # ì´ì „ ë‹µë³€ ìš”ì•½
    history_text = "\n".join([
        f"Q: {qa['question']}\nA: {qa['answer'][:100]}..."
        for qa in qa_history[-3:]  # ìµœê·¼ 3ê°œë§Œ
    ])

    dimension_map = {
        "work_style": "ì—…ë¬´ ìŠ¤íƒ€ì¼",
        "problem_solving": "ë¬¸ì œ í•´ê²° ë°©ì‹",
        "learning": "í•™ìŠµ ì„±í–¥",
        "stress_response": "ìŠ¤íŠ¸ë ˆìŠ¤ ëŒ€ì‘",
        "communication": "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜"
    }

    # ì´ì „ ì‹œë„ ì‹¤íŒ¨ ì´ìœ  (ì²« ì‹œë„ê°€ ì•„ë‹ ë•Œë§Œ)
    previous_failure_context = ""
    if state["attempts"] > 0 and state.get("validation_errors"):
        previous_failure_context = f"""
**âš ï¸ ì´ì „ ì‹œë„ ì‹¤íŒ¨ ì´ìœ :**
{chr(10).join(f"- {err}" for err in state["validation_errors"])}

**í”¼ë“œë°±:**
{state.get("llm_feedback", "")}

**ì´ì „ì— ìƒì„±í•œ ì§ˆë¬¸ (ì‚¬ìš© ë¶ˆê°€):**
"{state.get("generated_question", "")}"

ğŸ‘‰ ìœ„ ì‹¤íŒ¨ ì´ìœ ë¥¼ ì°¸ê³ í•˜ì—¬ **ë‹¤ë¥¸ ê°ë„**ë¡œ ì ‘ê·¼í•˜ì„¸ìš”. ê°™ì€ ì£¼ì œë‚˜ ìœ ì‚¬í•œ ìƒí™©ì„ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”.
"""

    prompt = ChatPromptTemplate.from_messages([
        ("system", f"""ë‹¹ì‹ ì€ ì¸ì‚¬íŒ€ ì±„ìš© ë‹´ë‹¹ìì…ë‹ˆë‹¤.

        ì§€ì›ìì˜ [{dominant_trait}] ì„±í–¥ì„ ê¹Šì´ íŒŒì•…í•˜ê¸° ìœ„í•œ êµ¬ì²´ì ì¸ ì‹¬ì¸µ ì§ˆë¬¸ 1ê°œë¥¼ ìƒì„±í•˜ì„¸ìš”.

        ì´ì „ ë‹µë³€:
        {history_text}
        {previous_failure_context}

        **ì‹¬í™” ì§ˆë¬¸ ìƒì„± ê°€ì´ë“œ:**
        - {dimension_map.get(dimension, dimension)} ì°¨ì›ì—ì„œ [{dominant_trait}] ì„±í–¥ì„ ë” ê¹Šì´ í™•ì¸
        - **ì‹¤ì œ ê²½í—˜í–ˆë˜ êµ¬ì²´ì ì¸ ìƒí™©ì„ ë¬¼ì–´ë³´ê¸°** (ê°€ì • ì§ˆë¬¸ì´ ì•„ë‹ˆë¼ ê³¼ê±° ê²½í—˜ ì§ˆë¬¸)
        - ì´ì „ ë‹µë³€ì—ì„œ ì• ë§¤í–ˆë˜ ë¶€ë¶„ì„ ëª…í™•íˆ í•˜ë˜, ì´ì „ê³¼ ë¹„ìŠ·í•œ ë‚´ìš©ì„ ë¬»ì§€ ì•Šê¸°
        - **ê²½í—˜ì—ì„œ ì–´ë–¤ ê³ ë¯¼ì„ í–ˆê³ , ì™œ ê·¸ë ‡ê²Œ í–‰ë™í–ˆëŠ”ì§€ ì˜ì‚¬ê²°ì • ë°°ê²½ì„ ë“œëŸ¬ë‚´ë„ë¡ ì§ˆë¬¸**
        - ì˜ˆ/ì•„ë‹ˆì˜¤ë¡œ ë‹µí•  ìˆ˜ ì—†ëŠ” ì—´ë¦° ì§ˆë¬¸
        - íŠ¹ì • ì§êµ°ì— êµ­í•œë˜ì§€ ì•ŠëŠ” ë²”ìš©ì ì¸ ê²½í—˜ ì§ˆë¬¸
        - ì¸í„°ë·° ëŒ€ìƒìê°€ ì´í•´í•˜ê¸° ì‰½ê³  ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸
        - ëª¨ë“  ì§ˆë¬¸ì„ í•œê¸€ë¡œë§Œ ì‘ì„± (ì˜ì–´ ì§ˆë¬¸ ê¸ˆì§€)
        - **ì§ˆë¬¸ ê¸¸ì´ëŠ” 130ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±**

        **ì˜ˆì‹œ:**
        - ì£¼ë„í˜• â†’ "íŒ€ì´ë‚˜ ë¦¬ë”ì˜ ê²°ì •ì´ ì¡°ì§ ëª©í‘œì™€ ë§ì§€ ì•Šë‹¤ê³  ëŠë‚„ ë•Œ ì–´ë–»ê²Œ í–‰ë™í•˜ì‹œë‚˜ìš”? êµ¬ì²´ì ì¸ ì‚¬ë¡€ë¥¼ ë“¤ì–´ ë§ì”€í•´ì£¼ì„¸ìš”."
        - ë¶„ì„í˜• â†’ "ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ë‚˜ ë¬¸ì œë¥¼ ë§¡ì•˜ì„ ë•Œ, ë¬¸ì œì˜ ì›ì¸ì„ ë¶„ì„í•˜ê³  í•´ê²°ì±…ì„ ì„¤ê³„í•œ ê²½í—˜ì´ ìˆë‚˜ìš”? ê³¼ì •ê³¼ ê²°ê³¼ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë§ì”€í•´ì£¼ì„¸ìš”."
        - í˜‘ë ¥í˜• â†’ "íŒ€ ë‚´ ì˜ê²¬ì´ ê°ˆë ¸ì„ ë•Œ, ë‹¤ì–‘í•œ ê´€ì ì„ ì¡°ìœ¨í•˜ì—¬ í•©ì˜ë¥¼ ë„ì¶œí•œ ê²½í—˜ì´ ìˆë‚˜ìš”? ì‹¤ì œ í–‰ë™ê³¼ ê²°ê³¼ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
"""),
        ("user", f"[{dominant_trait}] ì„±í–¥ì„ ê¹Šì´ íŒŒì•…í•  ìˆ˜ ìˆëŠ” 'êµ¬ì²´ì ì¸' ìƒí™© ì§ˆë¬¸ 1ê°œë¥¼ ìƒì„±í•˜ì„¸ìš”.")
    ])

    # LLM í˜¸ì¶œ
    settings = get_settings()
    llm = ChatOpenAI(
        model="gpt-4.1-mini",
        temperature=0.5,
        api_key=settings.OPENAI_API_KEY
    )

    result = (prompt | llm).invoke({})

    # State ì—…ë°ì´íŠ¸
    state["generated_question"] = result.content
    state["attempts"] += 1

    print(f"[Generator] Generated question: {result.content[:50]}...")

    return state


# ==================== Validator Nodes ====================

class SituationalQuestionValidationResult(BaseModel):
    """LLM structured output for situational question validation"""
    is_valid: bool = Field(..., description="True if the question follows all guidelines")
    issues: List[str] = Field(default_factory=list, description="Detected issues or violations")
    reasoning: str = Field(..., description="Evaluation reasoning")


def validate_situational_question_llm_node(state: TalentSituationalQuestionState) -> TalentSituationalQuestionState:
    """LLM ê¸°ë°˜ ì˜ë¯¸ ê²€ì¦"""
    print("[Validator:LLM] Evaluating situational question with LLM")

    generated_question = (state["generated_question"] or "").strip()
    dominant_trait = state["dominant_trait"]
    dimension = state["dimension"]
    qa_history = state.get("qa_history") or []

    dimension_map = {
        "work_style": "ì—…ë¬´ ìŠ¤íƒ€ì¼",
        "problem_solving": "ë¬¸ì œ í•´ê²° ë°©ì‹",
        "learning": "í•™ìŠµ ì„±í–¥",
        "stress_response": "ìŠ¤íŠ¸ë ˆìŠ¤ ëŒ€ì‘",
        "communication": "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜"
    }
    dimension_keyword = dimension_map.get(dimension, dimension)

    # ì´ì „ Q ìš”ì•½
    prev_summary = "\n".join([
        f"{idx}. Q: {qa.get('question', '')[:120]}\n   A: {qa.get('answer', '')[:200]}"
        for idx, qa in enumerate(qa_history[-3:], 1)
    ]) or "ì—†ìŒ"

    prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ìƒí™© ë©´ì ‘ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§ˆë¬¸ì´ ì•„ë˜ ì¡°ê±´ì„ ì¶©ì¡±í•˜ëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.

ì¡°ê±´:
1. dominant_traitì™€ dimensionì„ ëª…í™•íˆ ê²¨ëƒ¥í•´ì•¼ í•©ë‹ˆë‹¤.
2. ì´ì „ ì§ˆë¬¸ê³¼ ì˜ë¯¸ì ìœ¼ë¡œ ì¤‘ë³µë˜ë©´ ì•ˆ ë©ë‹ˆë‹¤. 
3. ì§€ì›ìì˜ ì‹¤ì œ í–‰ë™ì„ ëŒì–´ë‚¼ ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ì—´ë¦° ì§ˆë¬¸ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
4. ì§ˆë¬¸ì´ ì¶©ë¶„íˆ ìì—°ìŠ¤ëŸ½ê³  íŠ¹ì • ì§êµ°ì— êµ­í•œë˜ì§€ ì•ŠëŠ” ë²”ìš©ì ì¸ ìƒí™©ìœ¼ë¡œ ë§¤ë„ëŸ½ê²Œ ì‘ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

ëª¨ë“  ì¡°ê±´ì´ ì¶©ì¡±ë˜ì§€ ì•Šìœ¼ë©´ is_valid=Falseë¡œ ë‘ê³  issuesì— ì´ìœ ë¥¼ ì ìœ¼ì„¸ìš”."""),
        ("user", f"""
[Dominant Trait] {dominant_trait}
[Dimension] {dimension_keyword}

[Generated Question]
{generated_question}

[Recent Q&A]
{prev_summary}
""")
    ])

    settings = get_settings()
    llm = ChatOpenAI(
        model="gpt-4.1-mini",
        temperature=0,
        api_key=settings.OPENAI_API_KEY
    ).with_structured_output(SituationalQuestionValidationResult)

    result = (prompt | llm).invoke({})

    state["validation_errors"] = list(result.issues or [])
    state["llm_feedback"] = result.reasoning
    state["is_valid"] = result.is_valid and not state["validation_errors"]

    if result.is_valid:
        print("[Validator:LLM] âœ… Semantic validation passed")
    else:
        print(f"[Validator:LLM] âŒ Semantic validation failed: {state['validation_errors']}")

    return state


def validate_situational_question_node(state: TalentSituationalQuestionState) -> TalentSituationalQuestionState:
    """ê¸°ë³¸ íœ´ë¦¬ìŠ¤í‹± ê²€ì¦"""
    print("[Validator:Heuristic] Running safety checks")

    errors = list(state.get("validation_errors", []))
    generated_question = (state["generated_question"] or "").strip()

    # ê¸¸ì´ ë° í•œê¸€ ë¹„ì¤‘
    if len(generated_question) < 20:
        errors.append("Question is too short (minimum 20 characters).")
    if len(generated_question) > 130:
        errors.append("Question is too long (maximum 130 characters).")

    english_chars = sum(1 for c in generated_question if 'a' <= c.lower() <= 'z')
    korean_chars = sum(1 for c in generated_question if 'ê°€' <= c <= 'í£')
    if korean_chars <= english_chars:
        errors.append("Question must be primarily written in Korean.")


    state["validation_errors"] = errors
    state["is_valid"] = state["is_valid"] and len(errors) == 0

    if state["is_valid"]:
        print("[Validator:Heuristic] âœ… Validation passed")
        state["final_question"] = generated_question
    else:
        print(f"[Validator:Heuristic] âŒ Validation failed: {errors}")

    return state


# ==================== Decision Logic ====================

def should_regenerate_situational(state: TalentSituationalQuestionState) -> Literal["regenerate", "finish"]:
    """
    ì¬ìƒì„± ì—¬ë¶€ ê²°ì •

    - ê²€ì¦ í†µê³¼: finish
    - ê²€ì¦ ì‹¤íŒ¨ + ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ë¯¸ë§Œ: regenerate
    - ê²€ì¦ ì‹¤íŒ¨ + ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ë„ë‹¬: finish (í˜„ì¬ ì§ˆë¬¸ ì‚¬ìš©)
    """
    max_attempts = 5

    if state["is_valid"]:
        print("[Decision] Question is valid. Finishing.")
        return "finish"

    if state["attempts"] >= max_attempts:
        print(f"[Decision] Max attempts ({max_attempts}) reached. Using current question anyway.")
        # ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ë„ë‹¬ ì‹œ í˜„ì¬ ì§ˆë¬¸ìœ¼ë¡œ ì§„í–‰
        state["final_question"] = state["generated_question"]
        return "finish"

    print(f"[Decision] Invalid. Regenerating... (attempt {state['attempts']}/{max_attempts})")
    return "regenerate"


# ==================== Graph êµ¬ì¶• ====================

def create_situational_deep_dive_question_graph() -> StateGraph:
    """
    Situational ì‹¬í™” ì§ˆë¬¸ ìƒì„± Graph

    Flow:
    START â†’ generator â†’ validator â†’ [decision] â†’ (regenerate â†’ generator) or (finish â†’ END)
    """
    workflow = StateGraph(TalentSituationalQuestionState)

    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("generator", generate_situational_deep_dive_question_node)
    workflow.add_node("validator_llm", validate_situational_question_llm_node)
    workflow.add_node("validator", validate_situational_question_node)

    # Edge ì¶”ê°€
    workflow.set_entry_point("generator")
    workflow.add_edge("generator", "validator_llm")
    workflow.add_edge("validator_llm", "validator")

    # Conditional Edge: validator í›„ ì¬ìƒì„± ë˜ëŠ” ì¢…ë£Œ
    workflow.add_conditional_edges(
        "validator",
        should_regenerate_situational,
        {
            "regenerate": "generator",
            "finish": END
        }
    )

    return workflow.compile()


# ==================== Public API ====================

def generate_deep_dive_question_with_graph(
    dominant_trait: str,
    dimension: str,
    qa_history: List[dict]
) -> str:
    """
    Situational ì‹¬í™” ì§ˆë¬¸ ìƒì„± (LangGraph ê¸°ë°˜)

    ê¸°ì¡´ talent/situational.pyì˜ generate_deep_dive_question()ë¥¼ ëŒ€ì²´í•˜ëŠ” í•¨ìˆ˜

    Args:
        dominant_trait: ê°€ì¥ ê°•í•œ ì„±í–¥ (ì˜ˆ: "ì£¼ë„í˜•", "ë¶„ì„í˜•")
        dimension: í•´ë‹¹ ì°¨ì› (ì˜ˆ: "work_style", "problem_solving")
        qa_history: ì´ì „ ì§ˆë¬¸-ë‹µë³€ ì´ë ¥

    Returns:
        ìƒì„±ëœ ì‹¬í™” ì§ˆë¬¸ (str)
    """
    print(f"\n{'='*60}")
    print(f"Situational Deep-Dive Question Generation (LangGraph)")
    print(f"Trait: {dominant_trait}, Dimension: {dimension}")
    print(f"{'='*60}\n")

    # ì´ˆê¸° State
    initial_state: TalentSituationalQuestionState = {
        "dominant_trait": dominant_trait,
        "dimension": dimension,
        "qa_history": qa_history,
        "generated_question": "",
        "validation_errors": [],
        "attempts": 0,
        "llm_feedback": "",
        "final_question": "",
        "is_valid": False
    }

    # Graph ì‹¤í–‰
    graph = create_situational_deep_dive_question_graph()
    final_state = graph.invoke(initial_state)

    print(f"\n{'='*60}")
    print(f"âœ… Generation Complete!")
    print(f"Attempts: {final_state['attempts']}")
    print(f"Valid: {final_state['is_valid']}")
    print(f"Question: {final_state['final_question'][:80]}...")
    print(f"{'='*60}\n")

    return final_state["final_question"]
