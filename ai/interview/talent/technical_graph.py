"""
LangGraph ê¸°ë°˜ Talent Technical ë™ì  ì§ˆë¬¸ ìƒì„±
ê¸°ì¡´ generate_personalized_question ë¡œì§ì„ LangGraphë¡œ ì „í™˜
"""

from typing import List, TypedDict, Literal
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field

from ai.interview.talent.models import (
    CandidateProfile,
    GeneralInterviewAnalysis,
    InterviewQuestion,
)
from config.settings import get_settings


# ==================== State ì •ì˜ ====================

def _format_question_list(all_questions: List[dict], limit: int = 7) -> str:
    """LLM í”„ë¡¬í”„íŠ¸ì— ì‚¬ìš©í•  ê°„ë‹¨í•œ ì§ˆë¬¸ ëª©ë¡"""
    if not all_questions:
        return "ì—†ìŒ"

    lines = []
    for item in all_questions[-limit:]:
        question = (item.get("question") or "").strip()
        skill = item.get("skill") or ""
        skill_label = f"[ê¸°ìˆ : {skill}]" if skill else "[ê¸°ìˆ  ì •ë³´ ì—†ìŒ]"
        qnum = item.get("question_number")
        if qnum:
            skill_label += f" Q{qnum}"
        lines.append(f"- {skill_label} {question}")
    return "\n".join(lines)


class TalentTechnicalQuestionState(TypedDict):
    """Talent Technical ë™ì  ì§ˆë¬¸ ìƒì„± State"""
    # Input
    skill: str  # í‰ê°€í•  ê¸°ìˆ 
    question_number: int  # 1, 2
    profile: CandidateProfile
    general_analysis: GeneralInterviewAnalysis
    previous_skill_answers: List[dict]  # í˜„ì¬ ê¸°ìˆ ì˜ ì´ì „ ë‹µë³€ë“¤
    all_previous_questions: List[dict]  # ì „ì²´ ê¸°ìˆ  Q&A

    # Process
    generated_question: InterviewQuestion
    validation_errors: List[str]
    attempts: int
    llm_feedback: str

    # Output
    final_question: InterviewQuestion
    is_valid: bool


# ==================== Generator Node ====================

def generate_talent_technical_question_node(state: TalentTechnicalQuestionState) -> TalentTechnicalQuestionState:
    """
    Talent Technical ì§ˆë¬¸ ìƒì„± ë…¸ë“œ

    ê¸°ì¡´ talent/technical.pyì˜ generate_personalized_question ë¡œì§ ì‚¬ìš©
    """
    print(f"[Generator] Generating Talent Technical question (attempt {state['attempts'] + 1})")

    skill = state["skill"]
    question_number = state["question_number"]
    profile = state["profile"]
    general_analysis = state["general_analysis"]
    previous_skill_answers = state["previous_skill_answers"] or []
    all_previous_questions = state.get("all_previous_questions") or []
    question_list_text = _format_question_list(all_previous_questions)

    # ì´ì „ ë‹µë³€ ì •ë¦¬
    prev_context = ""
    if previous_skill_answers:
        prev_context = "\n\n**ì´ì „ ì§ˆë¬¸ê³¼ ë‹µë³€:**\n"
        for i, ans in enumerate(previous_skill_answers, 1):
            prev_context += f"\n[ì§ˆë¬¸ {i}] {ans['question']}\n"
            prev_context += f"[ë‹µë³€] {ans['answer']}\n"
            if ans.get('feedback'):
                depth_areas = ans['feedback'].get('depth_areas', [])
                if depth_areas:
                    prev_context += f"[íŒŒê³ ë“¤ í¬ì¸íŠ¸] {', '.join(depth_areas)}\n"

    # ì´ì „ ì‹œë„ ì‹¤íŒ¨ ì´ìœ  (ì²« ì‹œë„ê°€ ì•„ë‹ ë•Œë§Œ)
    previous_failure_context = ""
    if state["attempts"] > 0 and state.get("validation_errors"):
        previous_failure_context = f"""
**âš ï¸ ì´ì „ ì‹œë„ ì‹¤íŒ¨ ì´ìœ :**
{chr(10).join(f"- {err}" for err in state["validation_errors"])}

**í”¼ë“œë°±:**
{state.get("llm_feedback", "")}

**ì´ì „ì— ìƒì„±í•œ ì§ˆë¬¸ (ì‚¬ìš© ë¶ˆê°€):**
ì§ˆë¬¸: "{state.get("generated_question", {}).get("question", "") if isinstance(state.get("generated_question"), dict) else (state.get("generated_question").question if state.get("generated_question") else "")}"
why: "{state.get("generated_question", {}).get("why", "") if isinstance(state.get("generated_question"), dict) else (state.get("generated_question").why if state.get("generated_question") else "")}"

ğŸ‘‰ ìœ„ ì‹¤íŒ¨ ì´ìœ ë¥¼ ì°¸ê³ í•˜ì—¬ **ë‹¤ë¥¸ ê°ë„**ë¡œ ì ‘ê·¼í•˜ì„¸ìš”. ê°™ì€ ì£¼ì œë‚˜ ìœ ì‚¬í•œ ì§ˆë¬¸ì„ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”.
"""

    # ì§ˆë¬¸ ë²ˆí˜¸ì— ë”°ë¥¸ ê°€ì´ë“œ
    if question_number == 1:
        depth_guide = """**1ë²ˆì§¸ ì§ˆë¬¸ (ê²½í—˜ ë„ì…):**
- êµ¬ì¡°í™” ë©´ì ‘ì—ì„œ ì–¸ê¸‰í•œ í•µì‹¬ ì£¼ì œë‚˜ ê´€ì‹¬ì‚¬ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°í•˜ì—¬ **ì‹¤ì œ ê²½í—˜**ì„ ë¬¼ì–´ë³´ê¸°
- ì˜ˆ1: "ì•„ê¹Œ ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì •ì— ê´€ì‹¬ì´ ë§ë‹¤ê³  í•˜ì…¨ëŠ”ë°, ì‹¤ì œë¡œ ë°ì´í„°ë¥¼ í™œìš©í•´ ì¤‘ìš”í•œ ê²°ì •ì„ ë‚´ë¦° ê²½í—˜ì´ ìˆë‚˜ìš”? ê·¸ë•Œ ì–´ë–¤ ë°°ê²½ê³¼ ìƒí™©ì´ì—ˆëŠ”ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”."
- ì˜ˆ2: "ë””ìì¸ í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©ì í”¼ë“œë°±ì„ ë°˜ì˜í–ˆë‹¤ê³  í•˜ì…¨ëŠ”ë°, êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ìƒí™©ì—ì„œ ì–´ë–¤ í”¼ë“œë°±ì„ ë°›ì•˜ê³ , ê·¸ê±¸ ì–´ë–»ê²Œ ë°˜ì˜í•˜ì…¨ë‚˜ìš”?"
- ì§€ì›ìì˜ ì „ë°˜ì ì¸ ì—­í• ê³¼ ê²½í—˜ì˜ ë°°ê²½, ë™ê¸°ë¥¼ êµ¬ì²´ì  ì‚¬ë¡€ë¡œ íƒìƒ‰
"""
    else:  # question_number == 2
        depth_guide = """**2ë²ˆì§¸ ì§ˆë¬¸ (ê³ ë¯¼ì˜ ê¹Šì´ì™€ í™•ì¥):**
- 1ë²ˆì§¸ ë‹µë³€ì—ì„œ ì–¸ê¸‰í•œ ê²½í—˜ì—ì„œ **ì–´ë–¤ ê³ ë¯¼ì„ í–ˆê³ , ì™œ ê·¸ ì„ íƒì„ í–ˆëŠ”ì§€, ê·¸ë¦¬ê³  ê·¸ ê²½í—˜ì„ í†µí•´ ë¬´ì—‡ì„ ë°°ì› ëŠ”ì§€** ê¹Šì´ íƒêµ¬
- ë‹¨ìˆœí•œ ì‚¬ë¡€ íšŒê³ ê°€ ì•„ë‹ˆë¼, 'ì–´ë–¤ ëŒ€ì•ˆì„ ê³ ë ¤í–ˆê³ , íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ì–´ë–»ê²Œ íŒë‹¨í–ˆìœ¼ë©°, ê·¸ ê²½í—˜ì´ ì´í›„ ì–´ë–»ê²Œ ë°œì „í–ˆëŠ”ê°€'ë¥¼ ëŒì–´ë‚´ëŠ” ë‹¨ê³„
- ì˜ˆ1: 1ë²ˆì—ì„œ "ë§ˆì¼€íŒ… ìº í˜ì¸ ì„±ê³¼ ë¶„ì„"ì„ ì–¸ê¸‰í–ˆë‹¤ë©´ â†’ "ì„±ê³¼ ì¸¡ì • ì‹œ ì–´ë–¤ ì§€í‘œë¥¼ ì„ íƒí•˜ì…¨ë‚˜ìš”? ë‹¤ë¥¸ ì§€í‘œëŠ” ê³ ë ¤í•˜ì§€ ì•Šìœ¼ì…¨ë‚˜ìš”? ê·¸ ì„ íƒì˜ ë°°ê²½ê³¼ íŠ¸ë ˆì´ë“œì˜¤í”„ëŠ” ë¬´ì—‡ì´ì—ˆë‚˜ìš”?"
- ì˜ˆ2: "ê·¸ ì ‘ê·¼ë²•ì„ ì„ íƒí•œ ì´ìœ ê°€ ìˆë‚˜ìš”? ë‹¤ë¥¸ ë°©ë²•ì´ë‚˜ ë„êµ¬ë„ ê²€í† í•˜ì…¨ë‚˜ìš”? ì–´ë–¤ ê¸°ì¤€ìœ¼ë¡œ ìµœì¢… ê²°ì •ì„ ë‚´ë¦¬ì…¨ë‚˜ìš”?"
- ì˜ˆ3: "ê·¸ ê²½í—˜ì„ í†µí•´ ë°°ìš´ ì ì´ë‚˜ ê¹¨ë‹¬ì€ ì ì´ ìˆë‹¤ë©´, ì´í›„ ë‹¤ë¥¸ í”„ë¡œì íŠ¸ì—ì„œ ì–´ë–»ê²Œ ì ìš©í•˜ì…¨ë‚˜ìš”?"
- ì˜ˆ4: "ë§Œì•½ ê°™ì€ ìƒí™©ì´ ë‹¤ì‹œ ì˜¨ë‹¤ë©´, ì–´ë–¤ ë¶€ë¶„ì„ ë‹¤ë¥´ê²Œ ì ‘ê·¼í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”? ëŒì´ì¼œë³´ë©´ ì–´ë–¤ ê³ ë¯¼ì´ ë” í•„ìš”í–ˆì„ê¹Œìš”?"
- ëª©í‘œ: ì§€ì›ìì˜ ì˜ì‚¬ê²°ì • ê³¼ì •, ê³ ë¯¼ì˜ ê¹Šì´, ì‚¬ê³  ìˆ˜ì¤€, ì„±ì°° ëŠ¥ë ¥, ì„±ì¥ ê°€ëŠ¥ì„± íŒŒì•…
"""

    # í”„ë¡œí•„ì—ì„œ ì •ë³´ ì¶”ì¶œ
    job_category = profile.basic.tagline if profile.basic else ""

    # ê²½ë ¥ ì •ë³´ì—ì„œ ê¸°ìˆ  ìŠ¤íƒ ì¶”ì¶œ
    skills: List[str] = []
    for exp in profile.experiences:
        if exp.summary:
            skills.extend([kw.strip() for kw in exp.summary.split(',') if kw.strip()])

    # ì¤‘ë³µ ì œê±° ë° ê¸¸ì´ ì œí•œ
    unique_skills = sorted({s for s in skills if s})
    skills = unique_skills[:10]

    # ì´ ê²½ë ¥ ê³„ì‚°
    total_experience = sum((exp.duration_years or 0) for exp in profile.experiences)

    # ê²½ë ¥ì‚¬í•­ ìš”ì•½
    experience_entries = []
    for exp in profile.experiences:
        duration_text = f"{exp.duration_years}ë…„" if exp.duration_years is not None else "ê¸°ê°„ ì •ë³´ ì—†ìŒ"
        experience_entries.append(f"- {exp.company_name} / {exp.title} ({duration_text})")
    experience_summary = "\n".join(experience_entries)

    # í™œë™ ìš”ì•½
    activities_summary = "\n".join([
        f"- {act.name} ({act.category}): {act.description or ''}"
        for act in profile.activities
    ])

    prompt = ChatPromptTemplate.from_messages([
        ("system", f"""ë‹¹ì‹ ì€ {job_category} ì§êµ°ì˜ ì‹¤ë¬´ì§„(ì§ë¬´ ì í•©ì„± ë©´ì ‘) ë©´ì ‘ê´€ìœ¼ë¡œ,
        ì§€ì›ìì˜ í”„ë¡œí•„ê³¼ ì´ì „ ì§ˆë¬¸ ëª©ë¡ì„ ì°¸ê³ í•˜ì—¬, ì§ë¬´ ì—­ëŸ‰ì„ ê²€ì¦í•˜ê¸° ìœ„í•œ ì—´ë¦° ë©´ì ‘ ì§ˆë¬¸ì„ ë§Œë“¤ì–´ ì£¼ì„¸ìš”.

        **ì§€ì›ì í”„ë¡œí•„:**
        - ì´ë¦„: {profile.basic.name if profile.basic else "ì§€ì›ì"}
        - ì§ë¬´: {job_category}
        - ì´ ê²½ë ¥: {total_experience}ë…„

        **ê²½ë ¥ì‚¬í•­:**
        {experience_summary}

        **í™œë™/í”„ë¡œì íŠ¸:**
        {activities_summary}

        **ì¶”ì¶œëœ ê¸°ìˆ  í‚¤ì›Œë“œ:**
        {", ".join(skills) if skills else "ì •ë³´ ì—†ìŒ"}

        **êµ¬ì¡°í™” ë©´ì ‘ì—ì„œ íŒŒì•…ëœ íŠ¹ì„±:**
        - ì£¼ìš” í…Œë§ˆ: {", ".join(general_analysis.key_themes)}
        - ê´€ì‹¬ ë¶„ì•¼: {", ".join(general_analysis.interests)}
        - ê°•ì¡°í•œ ê²½í—˜: {", ".join(general_analysis.emphasized_experiences)}
        - ì—…ë¬´ ìŠ¤íƒ€ì¼: {", ".join(general_analysis.work_style_hints)}
        - ì–¸ê¸‰í•œ ê¸°ìˆ : {", ".join(general_analysis.technical_keywords)}

        **ì§ˆë¬¸ ìƒì„± ë°©ë²•:**
        - **ì§ë¬´ ì—­ëŸ‰ ì„ ì •** : ì§€ì›ìì˜ ì§ë¬´ì— ì í•©í•˜ê³  í•„ìš”í•œ ì—­ëŸ‰ì„ 6ê°œ ì„ ì •
        - **ì—­ëŸ‰ ìˆ˜ì¤€ ì²´ê³„** ì •ì˜ : ê° ì—­ëŸ‰ë§ˆë‹¤ ì—­ëŸ‰ ìˆ˜ì¤€ ì²´ê³„(low/medium/high)ì„ êµ¬ì²´ì ì¸ ê¸°ì¤€ìœ¼ë¡œ ì œì‹œ
        - **ìˆ˜ì¤€ ì²´ê³„ ê¸°ë°˜ ì§ˆë¬¸ ìƒì„±** : ì—­ëŸ‰ ìˆ˜ì¤€ ì²´ê³„ë¥¼ ì°¸ê³ í•˜ì—¬ í‰ê°€ ê°€ëŠ¥í•˜ë„ë¡ ì ì ˆí•œ ì§ˆë¬¸ì„ ìƒì„±

        **ì§ˆë¬¸ ìƒì„± ì „ëµ:**
        {depth_guide}

        **ë²ˆí˜¸ ê·œì¹™:**
        - question_numberëŠ” í˜„ì¬ ê¸°ìˆ  ë‚´ ìˆœë²ˆ (1=ë„ì…, 2=ì‹¬í™”)ì…ë‹ˆë‹¤.

        **ì§ˆë¬¸ ì›ì¹™:**
        - ì—´ë¦° ì§ˆë¬¸ (ì§€ì›ìê°€ ì‹¤ì œ ê²½í—˜ì„ ë§í•  ìˆ˜ ìˆë„ë¡ ìœ ë„, ì‹¤ë¬´ ì¤‘ì‹¬ì˜ êµ¬ì²´ì ì¸ ì§ˆë¬¸)
        - ì‚¬ì‹¤ ê¸°ë°˜ ì§ˆë¬¸ (í”„ë¡œí•„ê³¼ ì¸í„°ë·° ë‹µë³€ì— ìˆëŠ” ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì—¬ ì ì ˆí•œ ì§ˆë¬¸ ìƒì„±, ì œì‹œë˜ì§€ ì•Šì€ ê²½í—˜ì„ ë§Œë“¤ì–´ì„œ ë¬¼ì–´ë³´ì§€ ë§ ê²ƒ)
        - ì¶”ì • ë° ê³¼ì¥ ê¸ˆì§€ (ì–¸ê¸‰ë˜ì§€ ì•Šì€ ë‚´ìš©ì„ ë§Œë“¤ì–´ë‚´ì§€ ë§ ê²ƒ)
        - ìœ ì‚¬ ì§ˆë¬¸ ê¸ˆì§€ (ì˜ë¯¸ì—†ì´ ë¹„ìŠ·í•œ ì§ˆë¬¸ì„ í•˜ëŠ” ê²ƒì€ ì§€ì–‘)
        - ì•„ë˜ ì§ˆë¬¸ ëª©ë¡(ì´ë¯¸ ì‚¬ìš©í•œ ì§ˆë¬¸)ê³¼ ìœ ì‚¬í•œ í‘œí˜„/ì£¼ì œë¥¼ ê·¸ëŒ€ë¡œ ë°˜ë³µí•˜ì§€ ë§ ê²ƒ
        - ì¶”ê°€ ì§ˆë¬¸ì¼ ê²½ìš° ì´ì „ ë‹µë³€ì—ì„œ ì–¸ê¸‰ëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë” êµ¬ì²´ì ì´ê³  ê¹Šì´ ìˆëŠ” í›„ì† ì§ˆë¬¸ì„ ìƒì„±
        - ëª¨ë“  ì§ˆë¬¸ì„ í•œê¸€ë¡œë§Œ ì‘ì„± (ì˜ì–´ ì§ˆë¬¸ ê¸ˆì§€)
        - **ì§ˆë¬¸ ê¸¸ì´ëŠ” 130ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±** (í•µì‹¬ë§Œ ë‹´ì•„ ëª…í™•í•˜ê²Œ ì „ë‹¬)

        **ì˜ˆì‹œ:**
        - ìƒˆë¡œìš´ êµìœ¡ í”„ë¡œê·¸ë¨ì„ ì„¤ê³„í•  ë•Œ, í•™ìŠµì ìš”êµ¬ë‚˜ ì¡°ì§ì˜ ëª©í‘œë¥¼ ì–´ë–»ê²Œ ë°˜ì˜í•˜ì…¨ë‚˜ìš”? ì„¤ê³„ ê³¼ì •ì—ì„œ ì–´ë–¤ ì˜ì‚¬ê²°ì •ì„ ë‚´ë ¸ëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ ì£¼ì„¸ìš”.
        - ì´ì „ ë‹µë³€ì—ì„œ React í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ë©° Reduxë¥¼ ì‚¬ìš©í–ˆë‹¤ê³  ë‹µí•˜ì…¨ëŠ”ë°, Reduxë¥¼ ì„ íƒí•œ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”? ê·¸ ì„ íƒì´ í”„ë¡œì íŠ¸ êµ¬ì¡°ë‚˜ ì„±ëŠ¥ì— ì–´ë–¤ ì˜í–¥ì„ ì£¼ì—ˆëŠ”ì§€ë„ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
        """),
        ("user", f"""
í˜„ì¬ í‰ê°€ ê¸°ìˆ : {skill}
ì§ˆë¬¸ ë²ˆí˜¸: {question_number}/2
{prev_context}
{previous_failure_context}

**ì§€ê¸ˆê¹Œì§€ ì‚¬ìš©í•œ ì§ˆë¬¸ ëª©ë¡(ìµœëŒ€ 6ê°œ, ì´ë¯¸ ì§„í–‰í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤ / Qnì€ í•´ë‹¹ ê¸°ìˆ  ë‚´ ìˆœë²ˆ):**
{question_list_text}
â†’ ìœ„ ì§ˆë¬¸ì„ ë°˜ë³µí•˜ì§€ ë§ê³  ë‹¤ë¥¸ ê´€ì ìœ¼ë¡œ ì§ˆë¬¸í•˜ì„¸ìš”.
â†’ ìœ„ ëª©ë¡ê³¼ ë™ì¼/ìœ ì‚¬í•œ ì§ˆë¬¸ì„ ë°˜ë³µí•˜ì§€ ë§ê³ , ìƒˆë¡œìš´ ê°ë„ì˜ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.

{skill}ì— ëŒ€í•œ {question_number}ë²ˆì§¸ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.
""")
    ])

    # LLM í˜¸ì¶œ
    settings = get_settings()
    llm = ChatOpenAI(
        model="gpt-4.1-mini",
        temperature=0.5,
        api_key=settings.OPENAI_API_KEY
    ).with_structured_output(InterviewQuestion)

    result = (prompt | llm).invoke({})

    # State ì—…ë°ì´íŠ¸
    state["generated_question"] = result
    state["attempts"] += 1

    print(f"[Generator] Generated question: {result.question}")

    return state


# ==================== Validator Nodes ====================

class TechnicalQuestionValidationResult(BaseModel):
    """LLM structured output for question validation"""
    is_valid: bool = Field(..., description="True if the question follows all guidelines")
    issues: List[str] = Field(default_factory=list, description="Detected issues or violations")
    reasoning: str = Field(..., description="Brief reasoning of the evaluation")


def validate_talent_technical_question_llm_node(state: TalentTechnicalQuestionState) -> TalentTechnicalQuestionState:
    """LLM ê¸°ë°˜ ì˜ë¯¸ ê²€ì¦"""
    print("[Validator:LLM] Evaluating question semantics with LLM")

    generated_question = state["generated_question"]
    question_text = (generated_question.question or "").strip()
    why_text = (generated_question.why or "").strip()
    skill = state["skill"]
    question_number = state["question_number"]
    previous_skill_answers = state.get("previous_skill_answers") or []
    all_previous_questions = state.get("all_previous_questions") or []
    question_list_text = _format_question_list(all_previous_questions)

    # ì´ì „ ì§ˆë¬¸ ìš”ì•½
    prev_summary = ""
    if previous_skill_answers:
        prev_lines = []
        for idx, qa in enumerate(previous_skill_answers[-3:], 1):
            q = qa.get("question", "")[:120]
            a = qa.get("answer", "")[:200]
            prev_lines.append(f"{idx}. Q: {q}\n   A: {a}")
        prev_summary = "\n".join(prev_lines)
    else:
        prev_summary = "ì—†ìŒ"

    stage_intent = {
        1: "ì§€ì›ìì˜ ë°°ê²½/ë™ê¸°/ëŒ€í‘œ ê²½í—˜ì„ íŒŒì•…í•˜ëŠ” ë„ì… ì§ˆë¬¸",
        2: "ì´ì „ ë‹µë³€ì„ í† ëŒ€ë¡œ êµ¬ì²´ì ì¸ ë°©ë²•, íŒë‹¨ ê·¼ê±°, ë°°ìš´ ì , ì „ì´ ê°€ëŠ¥ì„±ì„ íŒŒê³ ë“œëŠ” ì‹¬í™” ì§ˆë¬¸"
    }.get(question_number, "ì¼ë°˜ì ì¸ í›„ì† ì§ˆë¬¸")

    prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ì¸í„°ë·°ì–´ì…ë‹ˆë‹¤. ì•„ë˜ ì§ˆë¬¸ì´ ê°€ì´ë“œë¼ì¸ì„ ì¶©ì¡±í•˜ëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.

ê²€ì¦ ê¸°ì¤€:
1. ì§ˆë¬¸ì´ ì§€ì •ëœ ê¸°ìˆ (skill)ì„ ëª…í™•íˆ ì–¸ê¸‰í•˜ê±°ë‚˜ í•´ë‹¹ ì—­ëŸ‰ì„ ê²¨ëƒ¥í•˜ëŠ”ê°€?
2. question_numberì— ë§ëŠ” ì¸í„°ë·° ì˜ë„ë¥¼ ì¶©ì¡±í•˜ëŠ”ê°€? (ë„ì…/ì‹¬í™”)
3. ì´ì „ ì§ˆë¬¸ê³¼ ì¤‘ë³µë˜ì§€ ì•Šê³ , ì´ì „ ë‹µë³€ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ê°€ëŠ”ê°€?
4. ì§ˆë¬¸ì´ ì¶©ë¶„íˆ êµ¬ì²´ì ì´ë©° ì§€ì›ìê°€ ì‹¤ì œ ê²½í—˜ì„ ì„¤ëª…í•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±ë˜ì–´ ìˆëŠ”ê°€?
5. 'why' ì„¤ëª…ì´ ì§ˆë¬¸ ëª©ì ì„ ëª…í™•íˆ ì„¤ëª…í•˜ëŠ”ê°€?

is_validê°€ Falseë¼ë©´ issuesì— ì´ìœ ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ë‚˜ì—´í•˜ì„¸ìš”.""" ),
        ("user", f"""
[Skill] {skill}
[Question Number] {question_number}
[Stage Intent] {stage_intent}

[Question]
{question_text}

[Why]
{why_text}

[Previous Q&A]
{prev_summary}

[Previously Asked Questions]
{question_list_text}
(ìœ„ ì§ˆë¬¸ë“¤ì€ ì´ë¯¸ ì‚¬ìš©ëœ íˆìŠ¤í† ë¦¬ì´ë©°, ê° Që²ˆí˜¸ëŠ” í•´ë‹¹ ê¸°ìˆ  ë‚´ ìˆœë²ˆì…ë‹ˆë‹¤. ë™ì¼/ìœ ì‚¬ ì§ˆë¬¸ì´ë©´ issuesì— ëª…ì‹œí•˜ì„¸ìš”.)
""")
    ])

    settings = get_settings()
    llm = ChatOpenAI(
        model="gpt-4.1-mini",
        temperature=0,
        api_key=settings.OPENAI_API_KEY
    ).with_structured_output(TechnicalQuestionValidationResult)

    result = (prompt | llm).invoke({})

    state["validation_errors"] = list(result.issues or [])
    state["llm_feedback"] = result.reasoning
    state["is_valid"] = result.is_valid and not state["validation_errors"]

    if result.is_valid:
        print("[Validator:LLM] âœ… Semantic validation passed")
    else:
        print(f"[Validator:LLM] âŒ Semantic validation failed: {state['validation_errors']}")

    return state


def validate_talent_technical_question_node(state: TalentTechnicalQuestionState) -> TalentTechnicalQuestionState:
    """
    ìµœì†Œ íœ´ë¦¬ìŠ¤í‹± ê²€ì¦ (í•œê¸€/ê¸¸ì´/ë¬¸ì¥ë¶€í˜¸ ë“±)
    """
    print("[Validator:Heuristic] Running basic safety checks")

    errors = list(state.get("validation_errors", []))
    generated_question = state["generated_question"]
    question_text = (generated_question.question or "").strip()
    why_text = (generated_question.why or "").strip()

    # ìµœì†Œ/ìµœëŒ€ ê¸¸ì´
    if len(question_text) < 20:
        errors.append("Question is too short (minimum 20 characters).")
    if len(question_text) > 130:
        errors.append("Question is too long (maximum 130 characters).")

    # í•œê¸€ ë¹„ì¤‘ ê²€ì‚¬
    english_chars = sum(1 for c in question_text if 'a' <= c.lower() <= 'z')
    korean_chars = sum(1 for c in question_text if 'ê°€' <= c <= 'í£')
    if korean_chars <= english_chars:
        errors.append("Question must be primarily written in Korean.")

    # why í•„ë“œ ê¸°ë³¸ ê¸¸ì´
    if len(why_text) < 10:
        errors.append("Why explanation is too short (minimum 10 characters).")

    state["validation_errors"] = errors
    state["is_valid"] = state["is_valid"] and len(errors) == 0

    if state["is_valid"]:
        print("[Validator:Heuristic] âœ… Validation passed")
        state["final_question"] = generated_question
    else:
        print(f"[Validator:Heuristic] âŒ Validation failed: {errors}")

    return state


# ==================== Decision Logic ====================

def should_regenerate_talent_technical(state: TalentTechnicalQuestionState) -> Literal["regenerate", "finish"]:
    """
    ì¬ìƒì„± ì—¬ë¶€ ê²°ì •

    - ê²€ì¦ í†µê³¼: finish
    - ê²€ì¦ ì‹¤íŒ¨ + ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ë¯¸ë§Œ: regenerate
    - ê²€ì¦ ì‹¤íŒ¨ + ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ë„ë‹¬: finish (í˜„ì¬ ì§ˆë¬¸ ì‚¬ìš©)
    """
    max_attempts = 5

    if state["is_valid"]:
        print("[Decision] Question is valid. Finishing.")
        return "finish"

    if state["attempts"] >= max_attempts:
        print(f"[Decision] Max attempts ({max_attempts}) reached. Using current question anyway.")
        # ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ë„ë‹¬ ì‹œ í˜„ì¬ ì§ˆë¬¸ìœ¼ë¡œ ì§„í–‰
        state["final_question"] = state.get("generated_question")
        return "finish"

    print(f"[Decision] Invalid. Regenerating... (attempt {state['attempts']}/{max_attempts})")
    return "regenerate"


# ==================== Graph êµ¬ì¶• ====================

def create_talent_technical_question_graph() -> StateGraph:
    """
    Talent Technical ë™ì  ì§ˆë¬¸ ìƒì„± Graph

    Flow:
    START â†’ generator â†’ validator â†’ [decision] â†’ (regenerate â†’ generator) or (finish â†’ END)
    """
    workflow = StateGraph(TalentTechnicalQuestionState)

    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("generator", generate_talent_technical_question_node)
    workflow.add_node("validator_llm", validate_talent_technical_question_llm_node)
    workflow.add_node("validator", validate_talent_technical_question_node)

    # Edge ì¶”ê°€
    workflow.set_entry_point("generator")
    workflow.add_edge("generator", "validator_llm")
    workflow.add_edge("validator_llm", "validator")

    # Conditional Edge: validator í›„ ì¬ìƒì„± ë˜ëŠ” ì¢…ë£Œ
    workflow.add_conditional_edges(
        "validator",
        should_regenerate_talent_technical,
        {
            "regenerate": "generator",
            "finish": END
        }
    )

    return workflow.compile()


# ==================== Public API ====================

def generate_personalized_question_with_graph(
    skill: str,
    question_number: int,
    profile: CandidateProfile,
    general_analysis: GeneralInterviewAnalysis,
    previous_skill_answers: List[dict] = None,
    all_previous_questions: List[dict] = None,
) -> InterviewQuestion:
    """
    Talent Technical ê°œì¸í™”ëœ ì§ˆë¬¸ ìƒì„± (LangGraph ê¸°ë°˜)

    ê¸°ì¡´ talent/technical.pyì˜ generate_personalized_question()ë¥¼ ëŒ€ì²´í•˜ëŠ” í•¨ìˆ˜

    Args:
        skill: í‰ê°€í•  ê¸°ìˆ 
        question_number: í•´ë‹¹ ê¸°ìˆ ì˜ ëª‡ ë²ˆì§¸ ì§ˆë¬¸ì¸ì§€ (1, 2, 3)
        profile: ì§€ì›ì í”„ë¡œí•„
        general_analysis: êµ¬ì¡°í™” ë©´ì ‘ ë¶„ì„ ê²°ê³¼
        previous_skill_answers: í˜„ì¬ ê¸°ìˆ ì˜ ì´ì „ ë‹µë³€ë“¤

    Returns:
        ìƒì„±ëœ ì§ˆë¬¸ (InterviewQuestion)
    """
    print(f"\n{'='*60}")
    print(f"Talent Technical Question Generation (LangGraph)")
    print(f"Skill: {skill}, Question #{question_number}")
    print(f"{'='*60}\n")

    # ì´ˆê¸° State
    initial_state: TalentTechnicalQuestionState = {
        "skill": skill,
        "question_number": question_number,
        "profile": profile,
        "general_analysis": general_analysis,
        "previous_skill_answers": previous_skill_answers or [],
        "all_previous_questions": all_previous_questions or [],
        "generated_question": None,
        "validation_errors": [],
        "attempts": 0,
         "llm_feedback": "",
        "final_question": None,
        "is_valid": False
    }

    # Graph ì‹¤í–‰
    graph = create_talent_technical_question_graph()
    final_state = graph.invoke(initial_state)

    final_question = final_state.get("final_question") or final_state.get("generated_question")
    if not final_question:
        raise RuntimeError("LangGraph ì§ˆë¬¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìƒì„±ëœ ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
    final_state["final_question"] = final_question

    print(f"\n{'='*60}")
    print(f"âœ… Generation Complete!")
    print(f"Attempts: {final_state['attempts']}")
    print(f"Valid: {final_state['is_valid']}")
    print(f"Question: {final_question.question[:80]}...")
    print(f"{'='*60}\n")

    return final_question
